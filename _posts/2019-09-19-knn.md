---
layout: post
title:  "K-近邻算法（KNN）"
date:   2019-09-19 11:10:10 +0800
# categories: 
tags: [ML]

---
k近邻法（k-Nearest Neighbor，简称kNN）是一种基本的分类与回归方法, 采用测量不同特征向量之间的距离的方法进行分类

* 分类问题：对新的样本，根据其k个最近邻的训练样本的类别，通过多数表决等方式进行预测。
* 回归问题：对新的样本，根据其k个最近邻的训练样本标签值的均值作为预测值。

工作原理：存在一个数据集，数据集中的每个数据都有对应的标签，当输入一个新的没有标签的数据时，KNN算法找到与新数据特征量最相似的分类标签。

* k近邻法不具有显式的学习过程，它是直接预测。它是惰性学习(lazy learning)的著名代表。
  1. 它实际上利用训练数据集对特征向量空间进行划分，并且作为其分类的"模型"。
  2. 这类学习技术在训练阶段仅仅将样本保存起来，训练时间开销为零，等到收到测试样本后再进行处理。那些在训练阶段就对样本进行学习处理的方法称作急切学习(eager learning)。

* k近邻法的三要素
  * k值选择
  * 距离度量
  * 决策规则

* KNN算法步骤
  1. 选择邻近的数量k和距离度量方法
  2. 找到待分类样本的k个最近邻居
  3. 根据最邻近的类标进行多数投票

* 优点：
  * k近邻模型具有非常高的容量，这使得它在训练样本数量较大时能获得较高的精度
  * 对异常值不敏感，无数据输入假定
* 缺点:
  * 计算成本很高。因为需要构建一个N×NN×N的距离矩阵，其计算量为O(N2)O(N^2)，其中N为训练样本的数量。当数据集是几十亿个样本时，计算量是不可接受的
  * 在训练集较小时，泛化能力很差，非常容易陷入过拟合
  * 无法判断特征的重要性
* 适用范围：数值型、标称型

#### KNeighborsClassifier 中的参数

|参数|含义|
|:------|:----|
|n_neighbors|k值，默认为5|
|weights|k个近邻样本的权重。可选值：uniform、distance，默认为uniform。uniform：所有最近邻样本权重一样；distance：权重和距离成反比；还可以自定义权重，即自定义一个函数，输入距离值，输出权重值。|
|algorithm|算法，可选值：'auto', 'ball_tree', 'kd_tree', 'brute'。'ball_tree'：球树实现；'kd_tree'：KD树实现；'brute’：蛮力实现；'auto'：会在上述三种算法中做权衡，选择一个拟合最好的最优算法。|
|leaf_size|用于控制KD树或球树停止建子树的叶子节点阈值，默认为30。这个值越小，生成的KD树或球树越大，层数越深，耗时越长。随着样本的增加，这个值要增加。|
|metric|距离度量，默认为'minkowski' 即闵可夫斯基距离。|
|p|距离度量参数metric附属参数，只用于'minkowski'中p值的选择，p=1,位曼哈顿距离，p=2位欧式距离,默认为2。|
|metric_params|距离度量的其他附属参数，主要用于带权重闵可夫斯基距离的权重，以及其他一些复杂的距离度量的参数。|
|n_jobs|并行处理任务数，默认值为1。|
